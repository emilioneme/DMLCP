{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6un0U90ipCJy",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "This is adapted from [this TF tutorial](https://www.tensorflow.org/tutorials/generative/dcgan), as well as these: [the Chollet notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part05_gans.ipynb), itself a port of [this Keras tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7Izwj47VpCJ4",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OpJXMzjPBFNP",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# reminder: Colab code to mount your drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # 'My Drive' is the default name of Google Drives\n",
    "    os.chdir('drive/My Drive/2023-DMLAP/DMLAP')\n",
    "    os.listdir() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Getting the CelebA dataset\n",
    "\n",
    "### Colab\n",
    "\n",
    "```bash\n",
    "!mkdir -p datasets/dcgan_celeba\n",
    "!gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O datasets/dcgan_celeba/data.zip\n",
    "!unzip -qq datasets/dcgan_celeba/data.zip -d datasets/dcgan_celeba\n",
    "```\n",
    "\n",
    "### Locally\n",
    "\n",
    "Create a directory called `datasets` then download manually from [here](https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684) (1.4 GB!) and unzip to a  (so your directory looks like `DMLAP/python/datasets/dcgan_celeba/img_align_celeba`.\n",
    "\n",
    "Or use [gdown](https://pypi.org/project/gdown/):\n",
    "\n",
    "```python\n",
    "!pip install gdown # in colab\n",
    "```\n",
    "```bash\n",
    " conda install -c conda-forge gdown # locally\n",
    "```\n",
    "\n",
    "With this code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celeb directory exists\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "celeba_dir = \"datasets/dcgan_celeba\"\n",
    "extracted_dir = os.path.join(celeba_dir, \"img_align_celeba\")\n",
    "\n",
    "if not os.path.isdir(extracted_dir):\n",
    "    print(\"Downloading Celeba dataset\")\n",
    "    os.makedirs(celeba_dir, exist_ok=True)\n",
    "\n",
    "    url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
    "    fname = \"datasets/dcgan_celeba/data.zip\"\n",
    "    gdown.download(url, fname, quiet=False)\n",
    "    \n",
    "    print(\"Unzipping\")\n",
    "    with ZipFile(\"datasets/dcgan_celeba/data.zip\", \"r\") as zipobj:\n",
    "        zipobj.extractall(\"datasets/dcgan_celeba\")\n",
    "else:\n",
    "    print(\"Celeb directory exists\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F738SsV3pCKU",
    "outputId": "66237c20-fa2c-4d2f-89e3-a356eae5eaeb",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "basedir = pathlib.Path(\"datasets/dcgan_celeba\")  # this will fail if you don't have a `dcgan_celeba` dir\n",
    "imgdir = basedir / \"img_align_celeba\"          # with, inside it, another folder containing the images\n",
    "outputdir = basedir / \"generated\"\n",
    "\n",
    "if not os.path.isdir(outputdir):\n",
    "    os.mkdir(outputdir)\n",
    "\n",
    "batch_size = 32 # 128\n",
    "img_size = 64\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    imgdir,\n",
    "    label_mode=None,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    smart_resize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202624 samples in 6332 batches\n"
     ]
    }
   ],
   "source": [
    "ds_len = len(dataset)\n",
    "print(f\"{ds_len * batch_size} samples in {ds_len} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limit the dataset**\n",
    "\n",
    "To make tests, and shorten the training time, you can limit the size of our dataset by defining a variable `num_batches` and use the `take()` method:\n",
    "```python\n",
    "num_batches = 300\n",
    "dataset_short = dataset.take(num_batches)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7stgdi7pCKY",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Rescaling the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v2q4sqmKpCKa",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x / 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY7EaDFwpCKb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Displaying the first image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "krE0axiKpCKc",
    "outputId": "58efd633-f7d1-40a5-8401-8ca65f7e4920",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tBmjf_3pCKn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# The generator\n",
    "\n",
    "In the Generator, the reverse operation of convolution is used to grow, rather than shrink, our image: the [`tf.keras.layers.Conv2DTranspose`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose). See [this Stack Overflow answer](https://drive.google.com/uc?id=1SnOH8oSc-Nm8BnfgBscZ9ZoreU4JFpxN) and [this article](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d) for more, and [this repo](https://github.com/vdumoulin/conv_arithmetic) for even more (summarised in [this video](https://www.youtube.com/watch?v=ilkSwsggSNM&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=137))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1vnSPBCmpCKp",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_dim = 100):\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(8 * 8 * 128),\n",
    "            tf.keras.layers.Reshape((8, 8, 128)),\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=\"generator\",\n",
    "    )\n",
    "\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPnzuRMrpCKq",
    "outputId": "0ba630f8-b2a6-4a4b-ce09-6a0d70584d28",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 8192)              1056768   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 128)       262272    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 256)       524544    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 512)       2097664   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 3)         38403     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3979651 (15.18 MB)\n",
      "Trainable params: 3979651 (15.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Zr7F4s3pCKg",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## The discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EQPTsQZpCKj",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(img_size, img_size, 3)),\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=\"discriminator\",\n",
    "    )\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bh1w7KfzpCKl",
    "outputId": "680a232d-faa8-4772-bc2c-d91c91786c4a",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 8193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 404801 (1.54 MB)\n",
      "Trainable params: 404801 (1.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNVW3oc0pCKu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kZRqVi3epCKv",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):                                                  # subclassing `tf.keras.Model`\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")           # custom metrics\n",
    "        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):                   # `compile` required for `tf.keras.Model`\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):                                       # `train_step` required for `tf.keras.Model`\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # 1. TRAIN DISCRIMINATOR --------------------------------------------------\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(                            # feed a batch of generated\n",
    "            shape=(batch_size, self.latent_dim)                             \n",
    "        )\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0) # & real images\n",
    "        labels = tf.concat( # (fake: 1, real: 0)\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n",
    "            axis=0\n",
    "        ) # (↓ label smoothing: inject randomness in the labels)\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        with tf.GradientTape() as tape:                                      # gradient logic:\n",
    "            predictions = self.discriminator(combined_images)                # Discriminator predicts\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)  # gradients to update\n",
    "        self.d_optimizer.apply_gradients(                                    # our Discriminator\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "        self.d_loss_metric.update_state(d_loss) # update loss\n",
    "\n",
    "        # 2. TRAIN GENERATOR ------------------------------------------------------\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(batch_size, self.latent_dim)\n",
    "        )\n",
    "        misleading_labels = tf.zeros((batch_size, 1)) # 0: real\n",
    "\n",
    "        with tf.GradientTape() as tape:                                 # gradient logic:\n",
    "            predictions = self.discriminator(                           # get predictions from Discriminator\n",
    "                self.generator(random_latent_vectors)                   # from generated fake images\n",
    "            )\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)       # loss labels vs preds\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights) # gradients to update\n",
    "        self.g_optimizer.apply_gradients(                               # our Generator\n",
    "            zip(grads, self.generator.trainable_weights)\n",
    "        )\n",
    "        self.g_loss_metric.update_state(g_loss) # update loss\n",
    "\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZoCQ5SlpCKy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A callback that samples generated images during training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XWgxNKSEpCK1",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.utils.array_to_img(generated_images[i])\n",
    "            img.save(outputdir / f\"generated_img_{epoch:03d}_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hnjkE6EpCK2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Compiling and training the GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "G7DCJDFz75am",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10 # ← Chollet has 100, you need a good GPU & patience for that!\n",
    "latent_dim = 100\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "\n",
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # very small learning rates!\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Reload a trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "G7DCJDFz75am",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reload = False # True to reload\n",
    "generator_path = basedir / \"generator_dcgan_celeba.h5\"\n",
    "discriminator_path = basedir / \"discriminator_dcgan_celeba.h5\"\n",
    "if os.path.isfile(discriminator_path) and os.path.isfile(generator_path) and reload:\n",
    "    gan.generator.load_weights(generator_path)\n",
    "    gan.discriminator.load_weights(discriminator_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pda0qVXwpCK2",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[\n",
    "        GANMonitor(num_img=10, latent_dim=latent_dim),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDzyVlZ3Y7W-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_gan_images(gan):\n",
    "    n = 12\n",
    "    random_latent_vectors = tf.random.normal(shape=(n, latent_dim))\n",
    "    generated_images = gan.generator(random_latent_vectors)\n",
    "    generated_images = tf.cast(generated_images * 255, tf.uint8)\n",
    "\n",
    "    # https://stackoverflow.com/a/54681765\n",
    "    _, axs = plt.subplots(3, 4, figsize=(12, 12))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(generated_images, axs):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(img.numpy())\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "gkncxRQTcMkV",
    "outputId": "091c6c72-c3df-433a-8235-7f994a6b6c3c",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_gan_images(gan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Save and reload a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model_path = basedir / \"generator_dcgan_celeba.keras\"\n",
    "discriminator_model_path = basedir / \"discriminator_dcgan_celeba.keras\"\n",
    "\n",
    "generator.save(generator_model_path)\n",
    "discriminator.save(discriminator_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model_path = basedir / \"gan_dcgan_celeba.keras\"\n",
    "GAN.save(gan_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_reloaded = tf.keras.models.load_model(generator_model_path)\n",
    "discriminator_reloaded = tf.keras.models.load_model(discriminator_model_path)\n",
    "\n",
    "gan_reloaded = GAN(                                     # REBUILD GAN\n",
    "    discriminator=discriminator_reloaded,\n",
    "    generator=generator_reloaded, \n",
    "    latent_dim=generator_reloaded.input_shape[1]        # the input shape is (batch, latent_dim) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "discriminator.save_weights(discriminator_path)          # SAVE\n",
    "generator.save_weights(generator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "discriminator_reloaded = build_discriminator()          # REBUILD D & G\n",
    "generator_reloaded = build_generator()\n",
    "\n",
    "discriminator_reloaded.load_weights(discriminator_path) # LOAD WEIGHTS\n",
    "generator_reloaded.load_weights(generator_path)\n",
    "\n",
    "gan_reloaded = GAN(                                     # REBUILD GAN\n",
    "    discriminator=discriminator_reloaded,\n",
    "    generator=generator_reloaded, \n",
    "    latent_dim=latent_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Notes / Tricks\n",
    "\n",
    "Here is a summary of some of the tricks Chollet mentions in his book, that are used in this implementation:\n",
    "\n",
    "- Sample from the latent space using a **normal distribution** (Gaussian), not a uniform one;\n",
    "- GANs are likely to get stuck in all sorts of ways (it's an unstable, dynamic equilibrium):  \n",
    "  we introduce **random noise** to the labels for the discriminator to prevent this;\n",
    "- Sparse gradients can hinder GAN training, remedy: **strided convolutions** for downsampling instead of max pooling, and the **`LeakyReLU`** instead of `ReLu`;\n",
    "- To avoid checkerboard artifacts caused by unequal coverage of the pixel space in the generator, use a kernel size **divisible by the stride size** with strided `Conv2DTranspose` or `Conv2D`.\n",
    "\n",
    "<small>*Deep Learning With Python*, 2<sup>nd</sup> ed., p.404</small>\n",
    "\n",
    "Note also that, as is mentioned by Chintala (see lecture above), the labels for true/fake are reversed from the original formulation (here 0 is true, 1 is fake), that seems to improve stability.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "The work that can be done here broadly falls into three main directions:\n",
    "- *Freeze* the network, work on the dataset:\n",
    "  - In this direction, most of your work is to gather datasets, and improve the ease of use. Are you able to develop a suite of tools that would allow you to handle datasets more easily? (In this case, the images are already cropped and the same size, which already takes some work! It would be nice to integrate tools that allow you to make this part of the work more streamlined: put any images in a folder, and a Python script crops them, etc.)? It might be worth looking into [data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) (inject randomness into your image dataset).\n",
    "  - It would be interesting to train GANs on generative images! You might end up with really distorted versions of what you started with.\n",
    "  - It's likely that people have trained GANs on spectrograms, as we see now with diffusion, but it might be a real fun thing to try?\n",
    "  - The image used for the week on text is a [book project by Allisson Parrish](https://www.aleator.press/releases/wendit-tnce-inf) that uses GANs to generate images of (unreadable) poems!\n",
    "  - Also, people have created loops where they train GANs on their own outputs, which creates distortions that may be worth exploring.\n",
    "- *Freeze* the dataset, work on the network:\n",
    "  - Maybe there's one dataset that's really your focus, or you're happy to work with established material (even MNIST!, like in the TF tutorial mentioned at the top), or the whole data processing feels boring? You might then want to look into fiddling with the model, and gather tricks (for instance: do you see an improvement if you normalise your images to be between [-1,1] instead of [0,1], like here (your Generator will have to have a `tanh` rather than a `sigmoid` as its last layer, see again the TF tutorial)? Then of course there's the network themselves, where all sorts of parameters can be tweaked, from the number of layers, to the strides of the convolution... Many GAN implementation add `BatchNormalization` layers in the generator, that could be tried (beware, you then need to explicitly pass `training=False` when calling your model for predictions, which disables the BatchNorm layers.\n",
    "  - **Note:** experimenting at a technical level with GANs (like with other things) can be a confusing rabbit hole. My recommendations are: make sure you have stable resources (e.g. you own a GPU or pay for Colab Pro), and try and make your net/dataset/experiments *as small/easy as possible*, so you can make a lot of them, get an inuition of what works and what doesn't. Perfect results really aren't the goal here, and it's never good for your momentum to have to wait hours or days before training finishes!\n",
    "  - How do you document this process of experimentation? You would probably need to save the various parameters of your experimentation (for yourself and, perhaps, the viewer), and associate that with some images generated at this point. In the TF tutorial, they use [imageio to create gifs](https://www.tensorflow.org/tutorials/generative/dcgan#create_a_gif).\n",
    "- *Freeze* both network and dataset, and try to use the network, or its output, in unexpected ways: one could imagine just training this network, or using a top-level StyleGAN (see below), and using the resulting images in some way, as material for something else? \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### The State of the Art\n",
    "\n",
    "The field has now moved away from GANs, as Diffusion has gained in popularity. The best results have probably been achieved by [Nvidia's StyleGan 3](https://nvlabs.github.io/stylegan3/) ([repo](https://github.com/NVlabs/stylegan3)). Check the [stylegan notebook](05_stylegan.ipynb) to check it out (on Colab!).\n",
    "\n",
    "Another interesting option could be lucidrainss [Lightweight GAN](https://github.com/lucidrains/lightweight-gan) implementation.\n",
    "\n",
    "### Zoos: list of all GAN variants\n",
    "\n",
    "When it comes to GANs, just like Diffusion now, the explosion has been so enormous it is rather difficult (impossible?) to keep up:\n",
    "\n",
    "- [Avinash Hindupur, \"The GAN Zoo\"](https://github.com/hindupuravinash/the-gan-zoo)\n",
    "- [Jihye Back, \"GAN-Zoos\"](https://happy-jihye.github.io/gan/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
