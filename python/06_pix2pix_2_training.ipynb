{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu0Wn4KjZAkI"
   },
   "source": [
    "# Pix2pix training\n",
    "\n",
    "Notebook adapted from [this tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct96WoFbZAkQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCPvlcadZAkW",
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Download a dataset, e.g.:\n",
    "\n",
    "- [Standard pix2pix datasets](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)\n",
    "- [Comic faces](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic)\n",
    "- [Maps](https://www.kaggle.com/datasets/alincijov/pix2pix-maps)\n",
    "- [Rembrandt](https://www.kaggle.com/datasets/grafstor/rembrandt-pix2pix-dataset)\n",
    "- [Depth](https://www.kaggle.com/datasets/greg115/pix2pix-depth)\n",
    "\n",
    "Or create your own using the notebook.\n",
    "\n",
    "Beware the sizes! Some of them are pretty big.\n",
    "\n",
    "## Colab workflow\n",
    "\n",
    "I recommend working in the same way as with the DCGAN:\n",
    "\n",
    "1. Either find a reliable url you can download a zip from, and use `!wget` directly in the notebook, followed by `unzip -d <target-dir> downloade-file.zip` (the standard pix2pix datasets would work this way).\n",
    "2. Or download the dataset locally first, then upload it to your drive, change the accessibility settings for the zip file in the drive, copy the `ID` and use `gdown` with the `ID`.\n",
    "\n",
    "Note: the standard pix2pix datasets can be unzipped using `tar -xf` (see [here](https://linuxize.com/post/how-to-extract-unzip-tar-gz-file/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCPvlcadZAkW",
    "tags": []
   },
   "source": [
    "## Load and preprocess dataset\n",
    "\n",
    "Let's first specify the path of our dataset and the desired image size (do not change the latter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fF6UiHGBZAkX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"datasets/edges2comics\" # Change this for your custom dataset\n",
    "model_dir = \"models/edges2comics\" # Can change. Model files and resulting images will be saved in this directory\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True) # this actually creates the directory if it does not exist\n",
    "\n",
    "generated_dir = os.path.join(model_dir , \"generated\") # For our generated pictures\n",
    "os.makedirs(generated_dir, exist_ok=True)\n",
    "\n",
    "BUFFER_SIZE = 400 # somewhat arbitrary, could be pushed up if memory allows\n",
    "BATCH_SIZE = 1\n",
    "IMG_SIZE = 256\n",
    "IMG_CHANNELS = 3\n",
    "IMG_EXTENSION = \".png\" # check directory and change to e.g. jpg instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-HopzNiZAkY"
   },
   "source": [
    "Each training image in a standard pix2pix dataset consists of one imgage divided into two adjacent **source** and **target** images.\n",
    "The layout of the source and target may vary from training set to trainig set, so we provide a `TARGET_INDEX` flag the determines on which side the target is (`0` if on the left and `1` if on the right). Set this so the examples from the dataset appear with the source image to the left.\n",
    "\n",
    "The following code also **augments** the dataset by applying random uniform scaling (by upscaling and cropping) and random mirroring to the input output pairs. This should lead to a more stable model according to the original pix2pix paper. Finally the images ar normalized to the [-1,1] range as required by our GAN-based model.\n",
    "\n",
    "We will organize the dataset in batches of size `1`, as that is generally suggested for pix2pix models. That means that we will update the weights of the model for each image pair separately.\n",
    "\n",
    "Run the code below and examine the resulting example images. Then set the `TARGET_INDEX` variable to reflect the position of the target image. That is `TARGET_INDEX = 0` if the target image is on the left and `TARGET_INDEX = 1` if it is on the right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NB73WZG-ZAkZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_INDEX = 1\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"Normalize the inputs to [-1, 1] (generator with 'tanh' activation)\"\"\"\n",
    "    return (x - 127.5) / 127.5\n",
    "\n",
    "def denorm(x):\n",
    "    \"\"\"Denormalize the outputs from [-1, 1] to [0,255] (generator with 'tanh' activation)\"\"\"\n",
    "    return (x + 1) * 127.5\n",
    "\n",
    "def resize(input_image, target_image, height, width):\n",
    "    input_image = tf.image.resize(\n",
    "        input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    target_image = tf.image.resize(\n",
    "        target_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    return input_image, target_image\n",
    "\n",
    "def random_crop(input_image, target_image):\n",
    "    stacked_image = tf.stack([input_image, target_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_SIZE, IMG_SIZE, 3])\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter(input_image, target_image):\n",
    "    # Resizing to 286x286, making room for cropping\n",
    "    input_image, target_image = resize(input_image, target_image, IMG_SIZE + 30, IMG_SIZE + 30)\n",
    "    # Random cropping back to 256x256\n",
    "    input_image, target_image = random_crop(input_image, target_image)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        target_image = tf.image.flip_left_right(target_image)\n",
    "    return input_image, target_image\n",
    "\n",
    "def load(image_file, target_index=1, img_channels=3):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.io.decode_image(image, channels=img_channels, expand_animations=False) # https://stackoverflow.com/a/59944421\n",
    "\n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with a real building facade image\n",
    "    # - one with an architecture label image\n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "\n",
    "    if target_index == 0:\n",
    "        input_image = image[:, w:, :]\n",
    "        target_image = image[:, :w, :]\n",
    "    else:\n",
    "        target_image = image[:, w:, :]\n",
    "        input_image = image[:, :w, :]\n",
    "\n",
    "    # Convert both images to float32 tensors & normalize\n",
    "    input_image = norm(tf.cast(input_image, tf.float32))\n",
    "    target_image = norm(tf.cast(target_image, tf.float32))\n",
    "\n",
    "    return input_image, target_image\n",
    "\n",
    "def load_and_preprocess(image_file):\n",
    "    input_image, target_image = load(image_file, target_index=TARGET_INDEX)\n",
    "    input_image, target_image = random_jitter(input_image, target_image)\n",
    "    return input_image, target_image\n",
    "\n",
    "dataset = tf.data.Dataset.list_files(os.path.join(dataset_dir, f\"*{IMG_EXTENSION}\"))\n",
    "dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "ito1QwkIsBZd",
    "outputId": "736cc0f2-51b1-4203-ae90-7ed4d21fbb9e"
   },
   "outputs": [],
   "source": [
    "for input_img, output_img in dataset.take(1):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"input\")\n",
    "    plt.imshow(denorm(input_img.numpy()).astype(\"int32\")[0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"target\")\n",
    "    plt.imshow(denorm(output_img.numpy()).astype(\"int32\")[0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Colab workflow\n",
    "\n",
    "To save generated images and model checkpoints to your drive, it's best to connect to your drive *after* downloading the dataset.\n",
    "\n",
    "```python\n",
    "# reminder: Colab code to mount your drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')  # 'My Drive' is the default name of Google Drives\n",
    "    os.chdir('drive/My Drive/2023-DMLAP/DMLAP/python') # change to your favourite dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU1NA1qsZAkb"
   },
   "source": [
    "## Build  the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F50usLYaZAkc"
   },
   "source": [
    "The pix2pix model is a conditional generative adversarial network (cGAN). A CGAN\n",
    "is a type of GAN model used for generating new data samples with specific\n",
    "attributes or characteristics. In a CGAN, both the generator and discriminator\n",
    "are *conditioned* on additional information, such as class labels, tags, or\n",
    "other types of metadata. The generator network takes in random noise as well as\n",
    "the conditional information as input and produces a new data sample that matches\n",
    "the desired attributes. The discriminator network, on the other hand, tries to\n",
    "distinguish between the generated samples and real samples based on both their\n",
    "visual appearance and the conditional information. For the case of a pix2pix\n",
    "model the network is conditioned on an image, which should be transformed into\n",
    "an output image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr7RGrPqZAkd"
   },
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2Y_7iSpZAke"
   },
   "source": [
    "Differently from a DC-GAN, the generator of the pix2pix model is based on the\n",
    "[U-net](https://arxiv.org/abs/1505.04597) architecture. A U-net model is a CNN architecture that is typically used\n",
    "for image segmentation tasks. The name U-net derives from the architecture,\n",
    "which resembles the letter &ldquo;U&rdquo;. It consists of two main parts: an *encoder* and\n",
    "a *decoder*. The encoder part consists of a series of convolutional layers,\n",
    "which reduce the spatial dimensionality of the input image while increasing its\n",
    "depth. This is followed by a bottleneck layer that extracts the most important\n",
    "features from the input image. The decoder part is a &ldquo;mirror image&rdquo; of the\n",
    "encoder. It consists of a series of layers that gradually increase the spatial\n",
    "dimensionality of the output, while decreasing its depth. This is similar to\n",
    "what we have seen in the DC-GAN example, but here we use a &ldquo;transposed\n",
    "convolution&rdquo; layer (`Conv2DTranspose`), while in the DCGAN example we used an\n",
    "upscaling followed by a convolution. The output of each layer in the encoder is\n",
    "also concatenated with the output of another layer in the decoder. This creates\n",
    "&ldquo;skip connections&rdquo; that help preserve spatial information and avoid information\n",
    "loss during the encoding and decoding process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2amP3ZGZAkf",
    "outputId": "454e8536-7c40-48f6-83d9-a0452d580166",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=init,\n",
    "            use_bias=False,\n",
    "        )\n",
    "    )\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=init,\n",
    "            use_bias=False,\n",
    "        )\n",
    "    )\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, 3])\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False), # (BATCH_SIZE, 128, 128, 64)\n",
    "        downsample(128, 4),                       # (BATCH_SIZE, 64, 64, 128)\n",
    "        downsample(256, 4),                       # (BATCH_SIZE, 32, 32, 256)\n",
    "        downsample(512, 4),                       # (BATCH_SIZE, 16, 16, 512)\n",
    "        downsample(512, 4),                       # (BATCH_SIZE, 8, 8, 512)\n",
    "        downsample(512, 4),                       # (BATCH_SIZE, 4, 4, 512)\n",
    "        downsample(512, 4),                       # (BATCH_SIZE, 2, 2, 512)\n",
    "        downsample(512, 4),                       # (BATCH_SIZE, 1, 1, 512)\n",
    "    ]\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),     # (BATCH_SIZE, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),     # (BATCH_SIZE, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),     # (BATCH_SIZE, 8, 8, 1024)\n",
    "        upsample(512, 4),                         # (BATCH_SIZE, 16, 16, 1024)\n",
    "        upsample(256, 4),                         # (BATCH_SIZE, 32, 32, 512)\n",
    "        upsample(128, 4),                         # (BATCH_SIZE, 64, 64, 256)\n",
    "        upsample(64, 4),                          # (BATCH_SIZE, 128, 128, 128)\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        IMG_CHANNELS, 4,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer=initializer,\n",
    "        activation='tanh'\n",
    "    )                                             # (BATCH_SIZE, IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # THE MODEL\n",
    "    # 1. Downsampling through the model\n",
    "    x = inputs\n",
    "    skips = [] # skip is for skip-connection = residual connections = allow information to pass unchanged through the net\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    # 2. Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KVJeglwVY4YF",
    "outputId": "c38283e3-3860-44f3-8aaf-7b9e1e040446"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(generator, to_file=f\"{model_dir}/generator.png\", show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r8TVlTFZAkg"
   },
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
    "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
    "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
    "- The discriminator receives 2 inputs:\n",
    "    - The input image and the target image, which it should classify as real.\n",
    "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
    "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d39MMY_ZAkh",
    "outputId": "fe4d2b68-56da-4648-95bd-485e3b580c3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, 3], name=\"input_image\")\n",
    "    tar = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, 3], name=\"target_image\")\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])             # (BATCH_SIZE, IMG_SIZE, IMG_SIZE, channels*2)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)                     # (BATCH_SIZE, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)                       # (BATCH_SIZE, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)                       # (BATCH_SIZE, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)      # (BATCH_SIZE, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "        512, 4,\n",
    "        strides=1,\n",
    "        kernel_initializer=init,\n",
    "        use_bias=False\n",
    "    )(zero_pad1)                                            # (BATCH_SIZE, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (BATCH_SIZE, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(\n",
    "        1, 4,\n",
    "        strides=1,\n",
    "        kernel_initializer=init\n",
    "    )(zero_pad2)                                            # (BATCH_SIZE, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(discriminator,  to_file=f\"{model_dir}/discriminator.png\", show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNIkm1vZZAki"
   },
   "source": [
    "### Generate some images before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l4MgsTvZAki"
   },
   "source": [
    "Let&rsquo;s generate some images before training to see what the network will output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "bkjY3Ks3ZAkj",
    "outputId": "681793b2-8f06-46d2-fff7-6c9aa4ab3594",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar, fname=''):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    if tar is not None:\n",
    "        display_list = [test_input[0], tar[0], prediction[0]]\n",
    "        title = ['Input Image', 'Target Image', 'Predicted Image']\n",
    "    else:\n",
    "        display_list = [test_input[0], prediction[0]]\n",
    "        title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(len(title)):\n",
    "        plt.subplot(1, len(title), i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 255] range to plot.\n",
    "        img = tf.cast(denorm(display_list[i]), tf.uint8)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # IDEA: this only saves the whole figure, instead you could save\n",
    "    # the target only!\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for example_input, example_target in dataset.take(1):\n",
    "    generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO1eQZSVZAkj"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Similarly to unconditional GANs, this conditional GAN (cGAN) is learning to map edges to photo.\n",
    "\n",
    "The discriminator D learns to classify between fake (synthesised by generator) and real {edges, photo} tuples. The generator G learns to fool the discriminator.\n",
    "\n",
    "Unlike an unconditional GAN, here, both G and D observe the input edge map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwe4lJJmZAkk"
   },
   "source": [
    "### Generator loss\n",
    "\n",
    "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "-   The `gan_loss` is a sigmoid cross-entropy loss of the generated images and an array of ones (ones = \"D says these images are true\", we want the generator to create images that yield this result!).\n",
    "-   The pix2pix paper also uses the L1 loss (`l1_loss`), which is a MAE (mean absolute error: \"how far apart are pixel values\") between the generated image and the target image: this forces the generated image to become structurally similar to the target image.\n",
    "-   The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100` (mixing the two losses). This value was decided by the authors of the paper.\n",
    "\n",
    "The full structure: \n",
    "\n",
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n",
    "\n",
    "Feel free to experiment with modifying the value of `LAMBDA` (if you have time to spare:))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73vGy39XZAkk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100 # lambda determines how strong we want the MAE to weigh in our full gen loss\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_fn(tf.ones_like(disc_generated_output), disc_generated_output) # measuring how much G fooled D (did D output ones?)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) # measuring how far G's output is from target (mean absolute error on pixels)\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ijt2pUZAkl"
   },
   "source": [
    "### Discriminator loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFRSCu6IZAkl"
   },
   "source": [
    "The discriminator loss function takes two inputs: real images and generated images.\n",
    "\n",
    "-   The `real_loss` is a sigmoid cross-entropy loss of the real images and an array of ones (since these are the real images).\n",
    "-   The `generated_loss` is a sigmoid cross-entropy loss of the generated images and an array of zeros (since these are the fake images).\n",
    "-   The `total_loss` is simply the sum of `real_loss` and `generated_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOingRyMZAkl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_fn(tf.ones_like(disc_real_output), disc_real_output)                 # D wants to predict 1s for real images\n",
    "    generated_loss = loss_fn(tf.zeros_like(disc_generated_output), disc_generated_output) # and wants to predict 0s for fake images\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHP5n87UZAkm"
   },
   "source": [
    "### Training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2WZJ7GeZAkm"
   },
   "source": [
    "The training loop procedes as follows:\n",
    "\n",
    "-   For each example input we generate an output.\n",
    "-   The discriminator receives the input image and the generated image as the first input. The second input is the input image and the target image.\n",
    "-   Next, calculate the generator and the discriminator loss.\n",
    "-   Then, calculate the gradients of the loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atHthNsdhQ6N"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_image, input_target):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "        \n",
    "        disc_real_output = discriminator([input_image, input_target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n",
    "            disc_generated_output, gen_output, input_target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "        generator_gradients = gen_tape.gradient(\n",
    "            gen_total_loss, generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(\n",
    "            disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss\n",
    "\n",
    "# Note: if you want to try and use TensorBoard for visualising your training,\n",
    "#       have a look at the tutorial this is inspired by for the code! The same\n",
    "#       goes for checkpointing (could be good technical practice, but fully optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0_kJb9D8FXP"
   },
   "outputs": [],
   "source": [
    "def plot(step, steps, d_losses, g_losses):\n",
    "    \"\"\"\n",
    "    Book-keeping: visualize losses and one example image for the epoch\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(f\"Losses | step: {step}/{steps}\")\n",
    "    plt.plot(np.array(d_losses) * 40, label=\"Discriminator\")\n",
    "    plt.plot(g_losses, label=\"Generator\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(model_dir, \"losses.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those will contain our losses (keeping them in a different cell allows for multiple restarts of the next one)\n",
    "g_losses = []\n",
    "d_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "XM6zmrGdZAkn",
    "outputId": "f8294542-53cf-4fe2-f87f-581334518f00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "STEPS = 40000\n",
    "PRINT_EVERY = 20\n",
    "SAVE_EVERY = 1000\n",
    "# ↑ try a lower number at first to see the results, e.g. 300, but when you train for a long time\n",
    "# you don't need to save all these models, and you could consider saving even less often (every\n",
    "# 10'000? One thing to keep in mind in that regard is that Colab tends to shut down sessions after\n",
    "# around 3h of inactivity, so make sure to save in an interval that is less than that!)\n",
    "\n",
    "for step, (input_image, input_target) in dataset.repeat().take(STEPS).enumerate():\n",
    "    \n",
    "    # our train step\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss = train_step(input_image, input_target)\n",
    "    \n",
    "    # save our losses\n",
    "    # note: it would be possible to perform a running average of those, instead of having the raw values\n",
    "    # (for that, set a factor gamma = .9, for instance, and save the first loss as is if the array is empty,\n",
    "    # and if the array isn't, save gamma*previous_loss + (1-gamma)*current_loss)(0 < gamma < 1, higher gamma = more smoothing)\n",
    "    g_l, d_l = gen_total_loss.numpy(), disc_loss.numpy()\n",
    "    g_losses.append(g_l)\n",
    "    d_losses.append(d_l)\n",
    "\n",
    "    if step > 0 and step % (PRINT_EVERY) == 0:\n",
    "        # clear_output(wait=True)\n",
    "        print(f\"Step {step:>{len(str(STEPS))}}/{STEPS} [G total loss: {g_l:.4f} | D loss: {d_l:.4f}]\")\n",
    "\n",
    "    # Save some example images and store model file\n",
    "    if step > 0 and step % SAVE_EVERY == 0:\n",
    "        plot(step, STEPS, d_losses, g_losses)\n",
    "        print(f\"Saving step {step} to {model_dir}\")\n",
    "        generate_images(\n",
    "            generator,\n",
    "            input_image,  # IDEA: here we use a different input each time, but you could instead import the logic\n",
    "            input_target, #       from the DCGAN and fix one input_image & input_target outside the loop, to see the evolution!\n",
    "            fname=os.path.join(generated_dir, f\"e{step:03}d_generated_image.png\")\n",
    "        )\n",
    "        generator.save(os.path.join(model_dir, f\"e{step:03}_generator.hd5\"), save_format='h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
