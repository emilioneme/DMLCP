{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "# Language Models 1\n",
    "\n",
    "## 1. Inference\n",
    "\n",
    "See [here](https://huggingface.co/tasks/text-generation) and [here](https://huggingface.co/docs/transformers/generation_strategies).\n",
    "\n",
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/2023-DMLAP/DMLAP/python') # change to your directory\n",
    "```\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. On Colab, you need to install `transformers`:\n",
    "\n",
    "```python\n",
    "!pip install -Uq transformers\n",
    "```\n",
    "\n",
    "2. Locally, I recommend creating a new environment when working with Huggingface, simply because it'll be faster and because the preferred library behind HF is PyTorch, which can conflict with TensorFlow...\n",
    "\n",
    "```\n",
    "conda create -n dmlap.hug python\n",
    "conda activate dmlap.hug\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "For some models, for instance the recent Llama 2, you need to be logged into HF.\n",
    "\n",
    "Please create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "In the case of Llama, you need two things: [request and be granted access by Meta](https://ai.meta.com/llama/), *then* [request and be granted access by HF](https://huggingface.co/meta-llama). With the former you can also download the weights locally (downloading all the models takes **a lot** of space, 330Gb, and most of them are literally unrunnable due to their size, so I wouldn't recommend it).\n",
    "\n",
    "```python\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URjvsuUyIthY"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Tf3fn2Z_DV"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    'text-generation', # the specific task, which is also the tag on huggingface\n",
    "    model='gpt2',      # so many more models here: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "    device=0           # the default is just cpu, see here: https://huggingface.co/docs/transformers/pipeline_tutorial#device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sueT3i6MfM8d"
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig.from_pretrained(\"gpt2\") # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.example\n",
    "generation_config.pad_token_id = generation_config.eos_token_id # see here, modified: https://github.com/huggingface/transformers/issues/19853#issuecomment-1290759818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVIs-VhOQN6O"
   },
   "source": [
    "The Huggingface is transitioning towards the use of generation config files (good for automation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fZaJdUnPg7E"
   },
   "outputs": [],
   "source": [
    "generation_config.max_length = 25\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.temperature = .9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW2UOpO2QAFD"
   },
   "source": [
    "### Quick vocab note:\n",
    "\n",
    "`bos`: beginning of sentence  \n",
    "`eos`: end of sentence  \n",
    "`pad`: padding\n",
    "\n",
    "These are special tokens that have been inserted into the text at training time.\n",
    "\n",
    "For instance, in our case the 'beginning' of the text is 'endoftext', as during training the texts are fed to the network one after the other, with this special token between them:\n",
    "```python\n",
    "print(generator.tokenizer.bos_token) # '<|endoftext|>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4NioPo5fSVu",
    "outputId": "84d70a38-4eee-4031-bac1-14a85467d6c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"_from_model_config\": true,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"max_length\": 25,\n",
       "  \"pad_token_id\": 50256,\n",
       "  \"temperature\": 0.9,\n",
       "  \"top_p\": 0.95,\n",
       "  \"transformers_version\": \"4.32.1\"\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61kgReONYBT5",
    "outputId": "cd2bae9a-2feb-44d7-c977-e371fea14c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Once upon a time, I'd just start playing games and play games. There was that same desire that, in other words\"}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.manual_seed(1)\n",
    "generator(\n",
    "    \"Once upon a time,\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McOEE98ITaoz"
   },
   "source": [
    "Parallel generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40-Zwu0ETWnW",
    "outputId": "34296884-51a8-46ff-baba-9ac0e1a8ff97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Once upon a time, one may think of our great heroes as \"good guys.\" After all, they\\'re the ones who'}],\n",
       " [{'generated_text': 'Once upon a time, he said, an individual in the United States is, as if someone had just arrived in the United'}]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.manual_seed(1)\n",
    "generator(\n",
    "    [\"Once upon a time,\"] * 2,\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "---\n",
    "\n",
    "## Deeper\n",
    "\n",
    "What does the pipeline do under the hood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EzS8SV5ccuu"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id # add the EOS token as PAD token to avoid warnings\n",
    ").to(\"cuda\") # to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6_9iKN7Wqps"
   },
   "source": [
    "### Note\n",
    "\n",
    "Huggingface automates everything, so instead of `GPT2LMHeadModel` and `GPT2Tokenizer` you can use `AutoModelForCausalLM`, `AutoTokenizer`.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(\"cuda\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXJjd_kEREkS"
   },
   "source": [
    "### The tokenizer\n",
    "\n",
    "See [this](https://huggingface.co/docs/transformers/preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5812, 6029, 15896]\n",
      "Oh sweet midnight\n",
      "\n",
      "{'input_ids': [[5812, 6029, 15896], [9869, 65, 3889, 286, 27666]], 'attention_mask': [[1, 1, 1], [1, 1, 1, 1, 1]]}\n",
      "['Oh sweet midnight', 'harbinger of doom']\n"
     ]
    }
   ],
   "source": [
    "toks = tokenizer.encode(\"Oh sweet midnight\")\n",
    "print(toks)\n",
    "print(tokenizer.decode(toks))\n",
    "print()\n",
    "\n",
    "toks = tokenizer([\"Oh sweet midnight\", \"harbinger of doom\"])\n",
    "print(toks)\n",
    "print(tokenizer.batch_decode(toks['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gju_L3gKVN4j",
    "outputId": "3914d075-337b-4850-a223-6fb6ac0cf7ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7454, 2402,  257,  640]])\n",
      "tensor([[7454, 2402,  257,  640],\n",
      "        [7454, 2402,  257,  640],\n",
      "        [7454, 2402,  257,  640],\n",
      "        [7454, 2402,  257,  640]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('Once upon a time', return_tensors='pt') # pytorch tensors\n",
    "print(input_ids)\n",
    "\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(\"cuda\") # just copying the tensor 4 times\n",
    "print(batched_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVKQUtN9eEpW"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('Once upon a time', return_tensors='pt') # pytorch tensors\n",
    "\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(\"cuda\") # copy and place on GPU\n",
    "\n",
    "# same logic as before\n",
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig.from_pretrained(\"gpt2\")\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "generation_config.max_length = 25\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.temperature = .9\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(\n",
    "    batched_input_ids, # try input_ids as well for a single strand\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibq2IrdhhSAH",
    "outputId": "4f4d119b-a32b-440a-d3ec-144a83d48e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, as the spirit guides you into the world, you will find that you have a strong sense of self\n",
      "----------------------------------------\n",
      "Once upon a time, it was a beautiful world, and so we saw it from it's window. He was always in\n",
      "----------------------------------------\n",
      "Once upon a time, when the universe is in a state of entropy, we have a series of infinite, infinite-dimensional\n",
      "----------------------------------------\n",
      "Once upon a time, I didn't think there were any problems with the system; now I see there are.\n",
      "\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "for t in texts:\n",
    "    print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "---\n",
    "\n",
    "# Experiments\n",
    "\n",
    "1. Test everything! Make sure you understand and develop an intuition of:\n",
    " - the various parameters: `temperature`, `top_k`, `top_p`;\n",
    " - the `tokenizer` object to convert text into tokens and back;\n",
    " - how to handle the whole pipeline;\n",
    "   Also, you can search for different [models](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)! (Some of them may exceed your GPU capacity, beware). People have finetuned language models on many types of texts.\n",
    "2. Can you think of a way to introduce computational thinking into this? Ideas:\n",
    "  - First, you could explore ways of making things look nicer? Instead of just having a list of objects? You could write a nice print function that knows exactly how to take the model output and print it in a nice way. The specialised Python package with many text functionalities is [textwrap](https://docs.python.org/3/library/textwrap.html) (see also [here](https://www.geeksforgeeks.org/textwrap-text-wrapping-filling-python/);\n",
    "  - Can you think of ways to construct a writing **loop**? By that, I mean:  \n",
    "    a. Prepare prompt  \n",
    "    b. Generate one or more strands of text  \n",
    "    c. Select text from strands, go back to a.  \n",
    "    This could simply mean writing a system of helper functions and classes to assist you in the writing...\n",
    "  - One could imagine all sorts of strange ways to work with text, from programmatically chunking the generated text and scrambling it before using it again as a prompt, to explore what the model does if you use unreasonable parameters (e.g. a very high or low `temperature`).\n",
    "  - Also, can you think of ways to work with various strands of text (Taking advantage of the fact that a model can generate in parallel)?\n",
    "\n",
    "3. Something that has already been the subject of a lot of debate and controversy, is the exploration of the *biases* of the models (and there are tons!). GPT-2 was trained mostly on Internet text, top-ranked reddit posts, etc. (see [this open-source replication](https://github.com/jcpeterson/openwebtext)). Unsurprisingly, the topics and points of view reflect that corner of human activities...\n",
    "\n",
    "### Document your thought process! Add more cells rather than modify the one you are working with, so that your steps can be retraced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXGhT-B9dqF6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
