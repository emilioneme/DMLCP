{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating your own pix2pix dataset\n",
    "=================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation requirements\n",
    "\n",
    "To run this notebook as well as the [07_visualizing_pix2pix_results.ipynb](07_visualizing_pix2pix_results.ipynb) notebook you might need to install some new Python packages. To do so, open a terminal and first make sure your environment is active:\n",
    "\n",
    "```bash\n",
    "conda activate dmlap\n",
    "conda install -c conda-forge pycairo opencv scikit-image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import filters\n",
    "from skimage import feature\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up \n",
    "Set your directories and the dataset specifics\n",
    "\n",
    "-   `TARGET_DIR` defines where your **target** images are located.\n",
    "-   `SOURCE_DIR` defines where your **source** images are located, if you already have these. Otherwise, set this to an empty string `''`.\n",
    "-   `DATASET_DIR` defines where your pix2pix dataset will be saved.\n",
    "-   `IS_INPUT_PIX_TO_PIX` set this to `True` if the input dataset already consists of an source and target pairs. This will be the case if you want to modify an existing pix2pix dataset. In this case we need to extract only the target.\n",
    "-   `INPUT_OUTPUT_TARGET_INDEX` if we are manipulating a dataset that is already a pix2pix dataset, this defines whether the target image is to the left (`0`) or to the right (`1`).\n",
    "\n",
    "Note you will have to put exactly the path to your image directories here, this code does not recursively search for images. Also note that the most common use case for this system will be with you providing an dataset of targets (desired outputs) that you will process to create the corresponding inputs (e.g. with edge detection or finding face landmarks). In that case you should not worry about the `SOURCE_DIR` directory below.\n",
    "\n",
    "Here, by default we will load the \"Face 2 comics\" dataset. Download the dataset from [Kaggle](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic), unzip, and place the `face2comics_v1.0.0_by_Sxela` directory in your dataset directory. This is already a \"pix2pix-friendly\" dataset consisting, however, of pairs of images that are separated. We will use the images to create an \"Edges to comics\" dataset, where we apply edge detection to a subset of the source images and leave the corresponding comic version unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_DIR = 'datasets/face2comics_v1.0.0_by_Sxela/comics/'\n",
    "SOURCE_DIR = None  # Only used if we already have source image examples, e.g. 'datasets/face2comics_v1.0.0_by_Sxela/face/'\n",
    "\n",
    "DATASET_DIR = 'datasets/bw2comics'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "IS_INPUT_PIX_TO_PIX = False\n",
    "INPUT_OUTPUT_TARGET_INDEX = 1 # 0: [target, source], 1: [source, target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the images to process\n",
    "\n",
    "Now let's load our target images, and optionally our source images if we have set the `SOURCE_DIR` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    size = (256, 256)\n",
    "    if IS_INPUT_PIX_TO_PIX: # In case we are already loading a pix2pix image\n",
    "        size = (256, 512)\n",
    "    img = image.load_img(path, target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    # If we are loading a pix2pix dataset just extract the target\n",
    "    if IS_INPUT_PIX_TO_PIX:\n",
    "        if INPUT_OUTPUT_TARGET_INDEX == 0:\n",
    "            img = img[:,:size[0],:]\n",
    "        else:\n",
    "            img = img[:,size[0]:,:]\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def load_images_in_path(path, shuffle=False, limit=0):\n",
    "    fnames = glob.glob(os.path.join(path, \"*\"))\n",
    "    print(f\"Found {len(fnames)} files in '{path}'\")    \n",
    "    if limit > 0:\n",
    "        fnames = fnames[:limit]\n",
    "        print(f\"Limiting number of files to {limit}\")\n",
    "    for f in fnames:\n",
    "        yield load_image(f) # See this: https://realpython.com/introduction-to-python-generators/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SOURCE_DIR:\n",
    "    source_loader = iter(load_images_in_path(SOURCE_DIR)) \n",
    "    plt.imshow(next(source_loader))\n",
    "    plt.show()\n",
    "\n",
    "target_loader = iter(load_images_in_path(TARGET_DIR)) # create an iterator\n",
    "plt.imshow(next(target_loader))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has a number of transformations already setup for you. These are:\n",
    "\n",
    "-  `apply_bw_cv2` Turns the picture to black and white (note: you need artificially to maintain the number of channels to three for the architecture to work).\n",
    "-   `apply_canny_cv2` Applies Canny edge detection by using OpenCV. You can set two parameters (thresholds between 0 and 255) that will determine the result of the edge detection: `thresh1` and `thresh2`. Experiment with these values to adjust the results to your liking. Additional details can be seen [here](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de).\n",
    "-   `apply_canny_skimage` Applies Canny edge detection by using [scikit-image](https://scikit-image.org). You can set one parameter, `sigma` that determines the number of edges. In general, a higher number will produce less edges. See [this](https://scikit-image.org/docs/stable/auto_examples/edges/plot_canny.html) for additional details.\n",
    "-   `apply_face_landmarks` Would face landmarks in an image by using [mlxtend](http://rasbt.github.io/mlxtend/) and uses the Canvas API to draw these as polygons. Note that this function will fail if the face detector cannot find a face in the image. **Removed**, as the recent mlxtend update stopped supporting facial landmarks. A nice task could be to reimplement this using ChatGPT.\n",
    "-   `load_source` simply gets the next source image from the directory specified in `SOURCE_DIR`.\n",
    "\n",
    "Set the `image_transformation` in the code below to the function that describes the transformation you want to apply.\n",
    "If you feel confident, you can extend this to other image transformations by duplicating one of the functions and adapting it to your needs.\n",
    "\n",
    "In the example below we will use the `apply_canny_cv` function. This means that we will load the source (face image) from the input dataset, and apply edge detection to that image for constructing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_bw_cv2(img, thresh1=160, thresh2=250):\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.merge([grey_img, grey_img, grey_img]) # Force three channels for shape compat, thanks ChatGPT!\n",
    "\n",
    "def apply_canny_cv2(img, thresh1=160, thresh2=250, invert=False):\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(grey_img, thresh1, thresh2)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, invert=False):\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grey_img, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# IDEA: rewrite this function using the Mediapipe API instead of mlxtend\n",
    "#       (ChatGPT I'm sure will be delighted to help you...)\n",
    "def apply_face_landmarks(img, stroke_weight=2):\n",
    "    raise NotImplementedError(\n",
    "        \"\"\"\n",
    "        This function used mlxtend.image.extract_face_landmarks, which has been removed from the library.\n",
    "        It would be worthwhile, if you are interested, to look at how to use the equivalent functionality\n",
    "        in MediaPipe to get the logic below to work again. See the following notebook:\n",
    "        https://github.com/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # # from py5canvas import canvas # if you installed canvas\n",
    "    # import canvas # if you have the canvas.py in the current directory\n",
    "    # from mlxtend.image import extract_face_landmarks\n",
    "\n",
    "    # def landmark_polylines(landmarks):\n",
    "    #     # https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/\n",
    "    #     landmarks = np.array(landmarks).astype(np.float32)\n",
    "    #     indices = [\n",
    "    #         list(range(0, 17)),         # Jawline\n",
    "    #         list(range(17, 22)),        # Left eyebrow\n",
    "    #         list(range(22, 27)),        # Right eyebrow\n",
    "    #         list(range(27, 31)),        # Nose bridge\n",
    "    #         list(range(31, 36)),        # Lower nose\n",
    "    #         list(range(36, 42)) + [36], # Left eye\n",
    "    #         list(range(42, 48)) + [42], # Right eye\n",
    "    #         list(range(48, 60)) + [48], # Outer lip\n",
    "    #         list(range(60, 68)) + [60]  # Inner lip\n",
    "    #     ]\n",
    "    #     return [landmarks[I] for I in indices]\n",
    "\n",
    "    # c = canvas.Canvas(256, 256)\n",
    "    # c.background(0)\n",
    "    # landmarks = extract_face_landmarks(img)\n",
    "    # if landmarks is None:\n",
    "    #     return None\n",
    "    # c.stroke_weight(stroke_weight)\n",
    "    # c.no_fill()\n",
    "    # c.stroke(255)\n",
    "    # paths = landmark_polylines(landmarks)\n",
    "    # for path in paths:\n",
    "    #     c.polyline(path)\n",
    "    # return c.get_image()\n",
    "\n",
    "# IDEA: It might be possible to use other Mediapipe functionalities, like:\n",
    "#       - segmentation: https://developers.google.com/mediapipe/solutions/vision/image_segmenter\n",
    "#       - pose landmarks: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
    "#       to write other transformation functions... (For both of those, you then need to find datasets!)\n",
    "\n",
    "# IDEA: Use Canvas (or openCV) to remove parts of the image (draw a rectangle/circle somewhere)\n",
    "#       so that the net learns to complete an image with a hole in it (inpainting)\n",
    "\n",
    "# As it is, this version loads an image from the source_image directory and applies the Canny edge detection\n",
    "# algorithm to it. Set transform=None if you just want to load that image without processing\n",
    "def load_source(img, img_source_iterator):\n",
    "    return next(img_source_iterator)\n",
    "\n",
    "# Set this to the tranformation you want to apply. If you are only working with a single folder of images that you\n",
    "# want to process, set image_transformation to one of the filtering operations above,\n",
    "# e.g. \n",
    "image_transformation = apply_bw_cv2\n",
    "# image_transformation = apply_canny_cv2\n",
    "# image_transformation = apply_canny_skimage\n",
    "\n",
    "# # If you are working with existing sources, you can use\n",
    "# # a Python partial to assign a fixed argument to load_source\n",
    "# # and use it exactly like the other image transformations\n",
    "# # (See: https://docs.python.org/3/library/functools.html#functools.partial)\n",
    "# from functools import partial\n",
    "# image_transformation = partial(load_source, img_source_iterator=iter(load_images_in_path(SOURCE_DIR)))\n",
    "  \n",
    "img = next(load_images_in_path(TARGET_DIR))\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_transformation(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset!\n",
    "\n",
    "Here we loop through all the target images, generate the source image and stitch these together into a single image. The input image directory might contain more than the desired number of images. If we want to process a lower number, set the `num_images` variable to a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_TARGET_INDEX = 1 # 0: [target, source], 1: [source, target]\n",
    "\n",
    "NUM_IMAGES = 0 # if you have a lot of images and want to parse only a fraction\n",
    "    \n",
    "img_loader = iter(load_images_in_path(TARGET_DIR, limit=NUM_IMAGES))\n",
    "\n",
    "def combine_images(source, target):\n",
    "    if OUTPUT_TARGET_INDEX == 1:\n",
    "        combined = np.hstack([source, target])\n",
    "    else:\n",
    "        combined = np.hstack([target, source])\n",
    "    return combined\n",
    "\n",
    "if SOURCE_DIR:\n",
    "    # CASE 1: we have source images, we want to combine them\n",
    "    # create a source image iterator\n",
    "    source_loader = iter(load_images_in_path(SOURCE_DIR, limit=num_images))   \n",
    "    # loop over both source and target and save the combine images\n",
    "    for i, (source, target) in enumerate(zip(source_loader, img_loader)):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Processing image {i}\")        \n",
    "        # IDEA: you could also apply additional processing to either your \n",
    "        # source or target here\n",
    "        combined = combine_images(source, target)\n",
    "        io.imsave(os.path.join(DATASET_DIR, f\"{i+1}.png\"), combined) \n",
    "else:\n",
    "    # CASE 2: we only have targets, we create the sources and combine them\n",
    "    for i, target in enumerate(img_loader):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Processing image {i}\")\n",
    "        \n",
    "        source = image_transformation(target)\n",
    "        if source is None:\n",
    "            print(f\"Failed to transform image {i+1}\")\n",
    "            continue\n",
    "\n",
    "        combined = combine_images(source, target)\n",
    "        io.imsave(os.path.join(DATASET_DIR, f\"{i+1}.png\"), combined)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
