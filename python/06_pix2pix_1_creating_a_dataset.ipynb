{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating your own pix2pix dataset\n",
    "=================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation requirements\n",
    "\n",
    "To run this notebook as well as the [07_visualizing_pix2pix_results.ipynb](07_visualizing_pix2pix_results.ipynb) notebook you might need to install some new Python packages. To do so, open a terminal and first make sure your environment is active\n",
    "```\n",
    "conda activate dmlap\n",
    "```\n",
    "and then\n",
    "```\n",
    "conda install -c conda-forge pycairo opencv scikit-image \n",
    "conda install -c conda-forge dlib mlxtend\n",
    "pip install pyglet\n",
    "```\n",
    "\n",
    "If you have not done so already, you will also need to install the [py5canvas](https://github.com/colormotor/py5canvas) module locally. To do so, navigate to a directory of choice in the terminal (you will install py5canvas in that directory) and then write:\n",
    "\n",
    "```\n",
    "git clone https://github.com/colormotor/py5canvas.git\n",
    "cd py5canvas\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### Updating py5canvas\n",
    "If you already installed py5canvas, you will need to updated it to the latest version. Get used to this because you will likely have to update it often as it is work in progress:) To do so, navigate to the `py5canvas` directory in the terminal (using `cd`) and then\n",
    "```\n",
    "git pull\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up \n",
    "Set your directories and the dataset specifics\n",
    "\n",
    "-   `target_dir` defines where your **target** images are located.\n",
    "-   `source_dir` defines where your **source** images are located, if you already have these. Otherwise, set this to an empty string `''`.\n",
    "-   `dataset_dir` defines where your pix2pix dataset will be saved.\n",
    "-   `is_input_pix_to_pix` set this to `True` if the input dataset already consists of an source and target pairs. This will be the case if you want to modify an existing pix2pix dataset. In this case we need to extract only the target.\n",
    "-   `input_target_index` if we are manipulating a dataset that is already a pix2pix dataset, this defines whether the target image is to the left (`0`) or to the right (`1`).\n",
    "\n",
    "Note you will have to put exactly the path to your image directories here, this code does not recursively search for images. Also note that the most common use case for this system will be with you providing an dataset of targets (desired outputs) that you will process to create the corresponding inputs (e.g. with edge detection or finding face landmarks). In that case you should not worry about the `source_dir` directory below.\n",
    "\n",
    "Here, by default we will load the \"Face 2 comics\" dataset. Download the dataset from [Kaggle](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic), unzip, and place the `face2comics_v1.0.0_by_Sxela` directory in your dataset directory. This is already a \"pix2pix-friendly\" dataset consisting, however, of pairs of images that are separated. We will use the images to create an \"Edges to comics\" dataset, where we apply edge detection to a subset of the source images and leave the corresponding comic version unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_dir = './datasets/face2comics_v1.0.0_by_Sxela/comics/'\n",
    "source_dir = './datasets/face2comics_v1.0.0_by_Sxela/face/'  # Only used if we already have source image examples\n",
    "\n",
    "dataset_dir = './datasets/edge2comics'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "is_input_pix_to_pix = False\n",
    "input_target_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the images to process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let&rsquo;s load our target images, and optionally our source images if we have set the `source_dir` directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def load_image(path):\n",
    "    size = (256, 256)\n",
    "    if is_input_pix_to_pix: # In case we are already loading a pix2pix image\n",
    "        size = (256, 512)\n",
    "    img = image.load_img(path, target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    # If we are loading a pix2pix dataset just extract the target\n",
    "    if is_input_pix_to_pix:\n",
    "        if input_target_index==0:\n",
    "            img = img[:,:size[0],:]\n",
    "        else:\n",
    "            img = img[:,size[0]:,:]\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "def load_images_in_path(path):\n",
    "    files = glob.glob(os.path.join(path, \"*\"))\n",
    "    images = []\n",
    "    for imgfile in tqdm(files): #, desc='Loading images in ' + path):\n",
    "        img = load_image(imgfile)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "target_images = load_images_in_path(target_dir)\n",
    "print(f'Loaded {len(target_images)} target images')\n",
    "if source_dir:\n",
    "    source_images = load_images_in_path(source_dir)\n",
    "    print(f'Loaded {len(source_images)} source images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has a number of transformations already setup for you. These are:\n",
    "\n",
    "-   `apply_canny_cv2` Applys Canny edge detection by using OpenCV. You can set two parameters (thresholds between 0 and 255) that will determine the result of the edge detection: `thresh1` and `thresh2`. Experiment with these values to adjust the results to your liking. Additional details can be seen [here](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de).\n",
    "-   `apply_canny_skimage` Applys Canny edge detection by using [scikit-image](https://scikit-image.org). You can set one parameter, `sigma` that determines the number of edges. In general, a higher number will produce less edges. See [this](https://scikit-image.org/docs/stable/auto_examples/edges/plot_canny.html) for additional details.\n",
    "-   `apply_face_landmarks` Finds face landmarks in an image by using [mlxtend](http://rasbt.github.io/mlxtend/) and uses the Canvas API to draw these as polygons. Note that this function will fail if the face detector cannot find a face in the image.\n",
    "-   `get_transformed` simply gets an already transformed image from the directory specified in `source_dir`.  This will not work if this directory is not specified. You can optionally set an additional transformation to the image by setting the `transform` parameter to one of the functions above. For example setting `transform=apply_canny_skimage` will apply the Canny edge detection algorithm to the loaded source image.\n",
    "\n",
    "Set the `image_transformation` in the code below to the function that describes the transformation you want to apply.\n",
    "If you feel confident, you can extend this to other image transformations by duplicating one of the functions and adapting it to your needs.\n",
    "\n",
    "In the example below we will use the `get_transformed` and set `transform=apply_canny_skimage`. This means that we will load the source (face image) from the input dataset, and apply edge detection to that image for constructing our dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage import feature, filters\n",
    "\n",
    "def apply_canny_cv2(index, img, thresh1=160, thresh2=250):\n",
    "    import cv2\n",
    "    invert = False\n",
    "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(grayimg, thresh1, thresh2)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def apply_canny_skimage(index, img, sigma=1.5):\n",
    "    import cv2\n",
    "    from skimage import feature\n",
    "    invert = False\n",
    "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grayimg, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def apply_face_landmarks(index, img, stroke_weight=2):\n",
    "    raise NotImplementedError(\n",
    "        \"\"\"\n",
    "        This function used mlxtend.image.extract_face_landmarks, which has been removed from the library.\n",
    "        It would be worthwhile, if you are interested, to look at how to use the equivalent functionality\n",
    "        in MediaPipe to get the logic below to work again. See the following notebook:\n",
    "        https://github.com/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # # from py5canvas import canvas # if you installed canvas\n",
    "    # import canvas # if you have the canvas.py in the current directory\n",
    "    # from mlxtend.image import extract_face_landmarks\n",
    "\n",
    "    # def landmark_polylines(landmarks):\n",
    "    #     # https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/\n",
    "    #     landmarks = np.array(landmarks).astype(np.float32)\n",
    "    #     indices = [\n",
    "    #         list(range(0, 17)),\n",
    "    #         list(range(17, 22)),\n",
    "    #         list(range(22, 27)),\n",
    "    #         list(range(27, 31)),\n",
    "    #         list(range(31, 36)),\n",
    "    #         list(range(36, 42)) + [36],\n",
    "    #         list(range(42, 48)) + [42],\n",
    "    #         list(range(48, 60)) + [48],\n",
    "    #         list(range(60, 68)) + [60]\n",
    "    #     ]\n",
    "    #     return [landmarks[I] for I in indices]\n",
    "\n",
    "    # c = canvas.Canvas(256, 256)\n",
    "    # c.background(0)\n",
    "    # landmarks = extract_face_landmarks(img)\n",
    "    # if landmarks is None:\n",
    "    #     return None\n",
    "    # c.stroke_weight(stroke_weight)\n",
    "    # c.no_fill()\n",
    "    # c.stroke(255)\n",
    "    # paths = landmark_polylines(landmarks)\n",
    "    # for path in paths:\n",
    "    #     c.polyline(path)\n",
    "    # return c.get_image()\n",
    "\n",
    "# As it is, this version loads an image from the source_image directory and applies the Canny edge detection\n",
    "# algorithm to it. Set transform=None if you just want to load that image without processing\n",
    "def load_transformed(index, img, transform=apply_canny_skimage):\n",
    "    if transform is not None:\n",
    "        return transform(index, source_images[index])\n",
    "    return source_images[index]\n",
    "\n",
    "# Set this to the tranformation you want to apply. If you are only working with a single folder of images that you\n",
    "# want to process, set image_transformation to one of the filtering operations above,\n",
    "# e.g. \n",
    "# image_tranformation = apply_canny_skimage\n",
    "\n",
    "image_transformation = load_transformed\n",
    "\n",
    "img = target_images[0]\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_transformation(0, img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset!\n",
    "\n",
    "Here we loop through all the target images, generate the source image and stitch these together into a single image. The input image directory might contain more than the desired number of images. If we want to process a lower number, set the `num_images` variable to a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "\n",
    "target_index = 1 # [target, source], 1: [source, target]\n",
    "num_images = 0\n",
    "shuffle = True\n",
    "image_indices = list(range(len(target_images)))\n",
    "if shuffle:\n",
    "    random.shuffle(image_indices)\n",
    "if num_images != 0:\n",
    "    image_indices = image_indices[:num_images]\n",
    "\n",
    "for i in tqdm(image_indices, desc=f\"Saving dataset to {dataset_dir}\"):\n",
    "    target = target_images[i]\n",
    "    source = image_transformation(i, target)\n",
    "    if source is None:\n",
    "        print(f\"Failed to transform image {i+1} of {len(image_indices)}\")\n",
    "        continue\n",
    "\n",
    "    if target_index == 1:\n",
    "        combined = np.hstack([source, target])\n",
    "    else:\n",
    "        combined = np.hstack([target, source])\n",
    "    io.imsave(os.path.join(dataset_dir, f\"{i+1}.png\"), combined)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
