{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6un0U90ipCJy",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Training a DCGAN II\n",
    "## (Deep Convolutional Generative Adversarial Networks)\n",
    "\n",
    "This is adapted from [this TF tutorial](https://www.tensorflow.org/tutorials/generative/dcgan), as well as these: [the Chollet notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part05_gans.ipynb), itself a port of [this Keras tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/), and [the paper](https://arxiv.org/pdf/1511.06434.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6un0U90ipCJy",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Install Imageio (to generate GIFs at the end)\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge imageio # locally (ships with Colab)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfIk2es3hJEd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import PIL\n",
    "import time\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) Dataset: Workflow\n",
    "\n",
    "### 1. Colab\n",
    "\n",
    "My recommendation is the following:\n",
    "\n",
    "1. Download the dataset from the Google Drive of the authors **once**, and upload the `img_align_celeba.zip` file (1.4 GB!) to your drive.  \n",
    "    1. Manually: [here](https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684)  \n",
    "    2. Using `gdown`: `!gdown 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684`\n",
    "\n",
    "2. Each time you train, download to **the cloud machine**, unzip and load the images from there (rather than from an unzipped version of your drive, it's *way* faster). Be nice, and use your Google drive version instead of the authors. For that, you will need to go to the file in your Google Drive, click the `â‹®` on the right, share, General access, \"Anyone with the link\". That link contains the id that you can use in in this code (or the equivalent code cell below):\n",
    "\n",
    "```bash\n",
    "!mkdir -p datasets/dcgan_celeba\n",
    "!gdown <ID-OF-YOUR-CELEBA-COPY> -O datasets/dcgan_celeba/img_align_celeba.zip\n",
    "!unzip -qq datasets/dcgan_celeba/img_align_celeba.zip -d datasets/dcgan_celeba\n",
    "```\n",
    "\n",
    "Perform these steps first, *then* connect to your drive and switch directories (if you want to save your model and generated images in your drive, otherwise no need).\n",
    "\n",
    "### 2. Locally\n",
    "\n",
    "Perform the steps to download the data once and unzip it so your directory looks like `DMLAP/python/datasets/dcgan_celeba/img_align_celeba` (using the lines above or the cell below).\n",
    "\n",
    "To install [gdown](https://pypi.org/project/gdown/): `conda install -c conda-forge gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FMYgY_mPfTi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "\n",
    "celeba_dir = \"datasets/dcgan_celeba\"\n",
    "extracted_dir = os.path.join(celeba_dir, \"img_align_celeba\")\n",
    "\n",
    "le_id = None # add your ID here\n",
    "\n",
    "if not os.path.isdir(extracted_dir): \n",
    "    if le_id is None:\n",
    "        print(\"Variable `le_id` is None: upload the Celeba dataset to your drive, retrieve its id, and add it to `le_id`!\")\n",
    "    else: \n",
    "        print(\"Downloading Celeba dataset\")\n",
    "        os.makedirs(celeba_dir, exist_ok=True)\n",
    "\n",
    "        fname = \"datasets/dcgan_celeba/data.zip\"\n",
    "        url = f\"https://drive.google.com/uc?id={le_id}\"\n",
    "        gdown.download(url, fname, quiet=False)\n",
    "\n",
    "        print(\"Unzipping\")\n",
    "        with ZipFile(\"datasets/dcgan_celeba/data.zip\", \"r\") as zipobj:\n",
    "            zipobj.extractall(\"datasets/dcgan_celeba\")\n",
    "else:\n",
    "    print(\"CelebA directory exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "Then you can import your files like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32    # you can push this up if you have a good GPU\n",
    "IMAGE_SIZE = 64    # reducing this could help speed up training, but the convnet\n",
    "                   # architecture would have to be adapted (in generator/discriminator)\n",
    "IMAGE_CHANNELS = 3 # 1 Grayscale (faster), 3 RGB\n",
    "IMAGE_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)\n",
    "\n",
    "LATENT_DIM = 100 # The size of the latent space/input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils \n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"Normalize the inputs to [-1, 1] (generator with 'tanh' activation)\"\"\"\n",
    "    return (x - 127.5) / 127.5\n",
    "\n",
    "def denorm(x):\n",
    "    \"\"\"Denormalize the outputs from [-1, 1] to [0,255] (generator with 'tanh' activation)\"\"\"\n",
    "    return (x + 1) * 127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Load and preprocess images](https://www.tensorflow.org/tutorials/load_data/images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FMYgY_mPfTi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_ds_dir = os.path.join(celeba_dir, \"img_align_celeba\")\n",
    "\n",
    "# this often fails on Colab (timeout reading file), just rerun the cell\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    image_ds_dir,\n",
    "    label_mode=None,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    smart_resize=True,\n",
    "    color_mode=\"rgb\" if IMAGE_CHANNELS == 3 else \"grayscale\" \n",
    ")\n",
    "train_dataset = train_dataset.map(lambda x: norm(x))  # normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_len = len(train_dataset)\n",
    "print(f\"{ds_len * BATCH_SIZE} samples in {ds_len} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let&rsquo;s see one random instance from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataset:\n",
    "    a = tf.cast(denorm(x[0]), tf.int32)\n",
    "    # print(tf.reduce_min(a), tf.reduce_max(a))\n",
    "    plt.imshow(a, cmap=\"gray\" if IMAGE_CHANNELS == 1 else None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "Now you could mount your drive and switch directory if you wanted to.\n",
    "\n",
    "```python\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')  # 'My Drive' is the default name of Google Drives\n",
    "    os.chdir('drive/My Drive/2023-DMLAP/DMLAP/python') # change to your favourite dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generator\n",
    "\n",
    "\n",
    "At this stage we need to construct our generator. There are many implementations out there, this is one that works reasonably well for our use case and it is adapted from [here](https://github.com/Kaustubh1Verma/Art-using-GANs/blob/ff41eeb5099d2aa3976ed1f051596d14015548d5/DCGAN/DCGAN.py).\n",
    "For GAN models it is [recommended](https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/) to initialize the layers with normally distributed (Gaussian) values and standard deviation of 0.02. The following function is built to enable training on different image sizes, but we recommend sticking with the `64x64` image size. You can vary the value of `kernel_size` to either `3`, `4` and `5` and see how that affects your image quality. A larger kernel size might produce better images in some cases, but will result in slower training. The code also has comments where you can experiment with modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_size = 3  # Set to either 3, 4 or 5\n",
    "\n",
    "# This will enable working with image sizes other than 64x64 (untested)\n",
    "upsample_layers = 5 # Needs to be the number of gen_block below\n",
    "start_size = IMAGE_SIZE // (2**upsample_layers)\n",
    "starting_filters = 128\n",
    "\n",
    "# This will initialize our layers randomly\n",
    "init = lambda: tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "# Define a generator convolutional block\n",
    "def gen_block(size, batch_norm=True):\n",
    "    return tf.keras.Sequential(\n",
    "        [   # UpSampling2D + Conv2D instead of Conv2DTranspose\n",
    "            tf.keras.layers.UpSampling2D(),\n",
    "            tf.keras.layers.Conv2D(size, kernel_size, padding=\"same\", kernel_initializer=init(), use_bias=False),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.9),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "generator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(LATENT_DIM,)),\n",
    "        tf.keras.layers.Dense(starting_filters * start_size * start_size, activation=\"relu\"),\n",
    "        tf.keras.layers.Reshape((start_size, start_size, starting_filters)),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.9),\n",
    "        gen_block(1024),\n",
    "        gen_block(512),\n",
    "        gen_block(128),\n",
    "        gen_block(64),\n",
    "        gen_block(32),\n",
    "        tf.keras.layers.Conv2D(IMAGE_CHANNELS, kernel_size=kernel_size, padding=\"same\",\n",
    "                               activation=\"tanh\", kernel_initializer=init()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "generator.summary(expand_nested=True, line_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s see it&rsquo;s output before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_vector = tf.random.normal([1, LATENT_DIM])\n",
    "generated_image = generator(latent_vector, training=True)\n",
    "generated_image = tf.cast(denorm(generated_image), tf.int32)\n",
    "print(generated_image.shape, generated_image.dtype, tf.reduce_min(generated_image), tf.reduce_max(generated_image))\n",
    "plt.imshow(generated_image[0], cmap=\"gray\" if IMAGE_CHANNELS == 1 else None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The discriminator\n",
    "\n",
    "We then define the discriminator model, again adapted adapted from [here](https://github.com/Kaustubh1Verma/Art-using-GANs/blob/ff41eeb5099d2aa3976ed1f051596d14015548d5/DCGAN/DCGAN.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a discriminator convolutional block\n",
    "def disc_block(size, strides, batch_norm=True, padding=False, dropout=False):\n",
    "    layer_list = [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            size, kernel_size=kernel_size, strides=strides, padding=\"same\",\n",
    "            kernel_initializer=init(), use_bias=not batch_norm\n",
    "        ),  # Always keep\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),  # Always keep\n",
    "    ]\n",
    "    if dropout:\n",
    "        layer_list += [ # Try varying the Dropout probabilty, keep the value <= 0.5\n",
    "            tf.keras.layers.Dropout(0.3)\n",
    "        ] \n",
    "    if padding:\n",
    "        layer_list += [ # Always keep this\n",
    "            tf.keras.layers.ZeroPadding2D(padding=((0, 1), (0, 1)))\n",
    "        ]\n",
    "    if batch_norm:\n",
    "        layer_list += [tf.keras.layers.BatchNormalization(momentum=0.8)]\n",
    "    return tf.keras.Sequential(layer_list)\n",
    "\n",
    "\n",
    "discriminator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=IMAGE_SHAPE),\n",
    "        tf.keras.layers.GaussianNoise(0.1),  # Injects noise into discriminator. Try varying noise amount <= 0.2, or removing by commenting this line\n",
    "        disc_block(32, 1, batch_norm=False),  # Try varying the dropout flag in these layers\n",
    "        disc_block(64, 2, batch_norm=True, padding=True),\n",
    "        disc_block(128, 2, batch_norm=True),\n",
    "        disc_block(256, 2, batch_norm=True),\n",
    "        disc_block(512, 2, batch_norm=False, dropout=True),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "discriminator.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment with changing this, lower values result in slower but more stable learning \n",
    "# You could also have two different losses for G/D, if you want to control the learning separately\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# on TF metal you might want to switch to tf.keras.optimizers.legacy.Adam\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5, beta_2=0.9) \n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "### Save checkpoints\n",
    "\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted. See [Save and load models](https://www.tensorflow.org/tutorials/keras/save_and_load) as well as [Training checkpoints](https://www.tensorflow.org/guide/checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FMYgY_mPfTi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = 'models/dcgan_celeba'\n",
    " \n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "generated_dir = os.path.join(model_dir , \"generated\")\n",
    "\n",
    "if not os.path.isdir(generated_dir):\n",
    "    os.mkdir(generated_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CA1w-7s2POEy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "    epoch=tf.Variable(0), # we save the epoch to be able to resume training\n",
    "    generator=generator,\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator=discriminator,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    ")\n",
    "\n",
    "manager = tf.train.CheckpointManager(checkpoint, os.path.join(model_dir, \"ckpt\"), max_to_keep=3)\n",
    "\n",
    "# This will automatically restore the latest checkpoint: you must delete the ckpt files for this not to happen\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def generate_images(generator, batch, latent_vectors=None, save=True):\n",
    "    if latent_vectors is None: # if no latent vector is passed, create one\n",
    "        latent_vectors = tf.random.normal(shape=(1, LATENT_DIM))\n",
    "    # Notice `training`= False, so that the model runs in inference mode (doesn't influcence training + batchnorm)\n",
    "    generated_images = generator(latent_vectors, training=False)\n",
    "    generated_images = tf.cast(denorm(generated_images), tf.int32)\n",
    "    \n",
    "    if save:\n",
    "        for i, gen_img in enumerate(generated_images):\n",
    "            img = tf.keras.preprocessing.image.array_to_img(gen_img)\n",
    "            img.save(os.path.join(generated_dir, f\"e{epoch+1:03}_{batch:04}_generated_img_{i+1}.png\"))\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "\n",
    "def plot(g_losses, d_losses, generator, generated_images=None, clear=False,\n",
    "         cmap=\"gray\" if IMAGE_CHANNELS == 1 else None, save=False):\n",
    "    \"\"\"\n",
    "    Book-keeping:\n",
    "    Visualize losses and one example image for the epoch\n",
    "    \"\"\"\n",
    "    if clear:  # You may want to clear the screen for longer training\n",
    "        display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    # losses\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Losses')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.legend()\n",
    "    \n",
    "    if generated_images is None:\n",
    "         generated_images = generate_images(generator, save=save)\n",
    "    # image\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(generated_images[0], cmap=cmap) # [0]: remove the batch dimension  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses & train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy() \n",
    "\n",
    "smoothing = 0.1   # Label smoothing (keep < 0.2), can improve results. Setting to zero will disable smoothing \n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # real_loss will quantify our loss to distinguish the real images\n",
    "    real_labels = tf.zeros_like(real_output) + smoothing * tf.random.uniform(tf.shape(real_output))\n",
    "    real_loss = cross_entropy(real_labels, real_output)\n",
    "\n",
    "    # fake_loss will quantify our loss to distinguish the fake images (generated)\n",
    "    fake_labels = tf.ones_like(fake_output) - smoothing * tf.random.uniform(tf.shape(fake_output))    \n",
    "    fake_loss = cross_entropy(fake_labels, fake_output)\n",
    "    \n",
    "    # Real image = 0, Fake image = 1 (array of ones and zeros)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss, real_loss, fake_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # We want the false images to be seen as real images (0s)\n",
    "    return cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "@tf.function # optimize for faster execution\n",
    "def train_step(images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    latent_vector = tf.random.normal([batch_size, LATENT_DIM])\n",
    "\n",
    "    # To make sure we know what is done, we will use a gradient tape instead of compiling\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Training the generator\n",
    "        generated_images = generator(latent_vector , training=True) \n",
    "\n",
    "        # Training the discriminator\n",
    "        real_output = discriminator(images, training=True)           # Training the discriminator on real images\n",
    "        fake_output = discriminator(generated_images, training=True) # Training the discriminator on fake images\n",
    "\n",
    "        # Calculating the losses\n",
    "        gen_loss =  generator_loss(fake_output)\n",
    "        disc_loss, disc_r_loss, disc_f_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        # Building the gradients\n",
    "        gradients_of_generator =     gen_tape.gradient(  gen_loss,  generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        \n",
    "        # Applying the gradients (backpropagation)\n",
    "        generator_optimizer.apply_gradients(    zip(gradients_of_generator,     generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        return gen_loss, disc_loss, disc_r_loss, disc_f_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop will save images and models for each epoch to a directory specifed in the variable `model_dir`. You can visualize the results by examining the directory. Keep note of this directory, because you will use that to load and visualize images in the [06_visualizing_gan_results.ipynb](05_visualizing_gan_results.ipynb) notebook. You may want to name the directory in a way that reminds you of the parameters you used for training. As usual, examine the code below and look out for comments that indicate paramters that you can modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60        # Number of epochs. Safe to increase but check what happens to the images\n",
    "\n",
    "save_every = 10    # (Epoch) How often to save the model\n",
    "gen_every = 50     # (Batch) How often we generate images (Colab is faster: 100?)\n",
    "print_every = 20   # (Batch) How often we print the loss (Colab is faster: 50?)\n",
    "\n",
    "max_batch = 300    # (Batch) skip to next epoch after limited number of batches, -1 to disable\n",
    "\n",
    "# Training loop\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "# Creating the vector once at the start provides some stability to our images\n",
    "# Leaving it as None will create different seeds at each iteration (can be good\n",
    "# to see if mode collapse is happening or not)\n",
    "init_latent_vectors = None # tf.random.normal([1, LATENT_DIM])\n",
    "\n",
    "start_epoch = checkpoint.epoch.numpy()\n",
    "\n",
    "n = train_dataset.cardinality().numpy().item() # The number of batches per epoch\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
    "    \n",
    "    # Iterate over all batches\n",
    "    batch_d_losses = []\n",
    "    batch_g_losses = []\n",
    "    for i, batch in enumerate(train_dataset): \n",
    "        \n",
    "        # limit computation\n",
    "        if i == max_batch:\n",
    "            break\n",
    "        \n",
    "        # Upadte parameters for this batch\n",
    "        g_loss, d_loss, r_loss, f_loss = train_step(batch)\n",
    "        \n",
    "        # Store losses for batch, we will average these for the whole epoch for a more stable visualization\n",
    "        batch_g_losses.append(g_loss)\n",
    "        batch_d_losses.append(d_loss)\n",
    "        \n",
    "        # Some printing\n",
    "        if i % print_every == 0:\n",
    "            dl, gl, rl, fl  = d_loss.numpy(), g_loss.numpy(), r_loss.numpy(), f_loss.numpy()\n",
    "            print(f\"epoch {epoch+1}, batch {i:{len(str(n))}/{n} [D loss: {dl:.4f} (real: {rl:.4f}, fake: {fl:.4f}) | G loss: {gl:.4f}]\")\n",
    "        \n",
    "        # Generating/Saving images\n",
    "        if i % gen_every == 0:\n",
    "            generated_images = generate_images(generator, i, latent_vectors=init_latent_vectors)\n",
    "             \n",
    "    print()\n",
    "    \n",
    "    g_losses.append(np.mean(batch_g_losses))\n",
    "    d_losses.append(np.mean(batch_d_losses))\n",
    "    \n",
    "    # Plot & save images\n",
    "    plot(g_losses, d_losses, generator, generated_images=generated_images)\n",
    "\n",
    "    # Saving model file (note the duplicate: this simplifies the loading in other files as we only use the generator\n",
    "    if epoch > 0 and epoch % save_every == 0:\n",
    "        print(f\"{epoch+1}, saving model to {model_dir}\")\n",
    "        manager.save()\n",
    "        generator.save(os.path.join(model_dir, f\"e{epoch+1:03}_generator_celeba.keras\")) # can be .h5\n",
    "\n",
    "    checkpoint.epoch.assign_add(1) # increment our epoch        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Create a GIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfO5wCdclHGL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no, img_no, resize_factor=6):\n",
    "    s = IMAGE_SIZE * resize_factor\n",
    "    return PIL.Image.open(\n",
    "        os.path.join(generated_dir, f\"e{epoch_no:03}_0000_generated_img_{img_no}.png\")\n",
    "    ).resize((s,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x3q9_Oe5q0A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_image(checkpoint.epoch.numpy(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NywiH3nL8guF"
   },
   "source": [
    "Use `imageio` to create an animated gif using the images saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as iio\n",
    "import glob\n",
    "\n",
    "anim_file = os.path.join(generated_dir, 'dcgan_celeba.gif')\n",
    "\n",
    "if os.path.isfile(anim_file):\n",
    "    print(f\"An existing {anim_file}, found, removing\")\n",
    "    os.remove(anim_file)\n",
    "\n",
    "n_image = 1       # I create only one series, but you could create several images / epoch\n",
    "resize_factor = 6 # Make our image bigger\n",
    "new_size = IMAGE_SIZE * resize_factor\n",
    "\n",
    "# adapting the the tutorial version to v3 + looping the gif (thanks ChatGPT)\n",
    "with iio.get_writer(anim_file, mode='I', loop=0) as writer:\n",
    "    filenames = glob.glob(os.path.join(generated_dir, f\"e*_generated_img_{n_image}.png\"))\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        # Use PIL to open and resize the image\n",
    "        with PIL.Image.open(filename) as img:\n",
    "            img_resized = img.resize((new_size, new_size))\n",
    "            writer.append_data(np.array(img_resized))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBwyU6t2Wf3g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from here: https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/vis/embed.py\n",
    "\n",
    "import base64\n",
    "import pathlib\n",
    "import mimetypes\n",
    "import IPython.display\n",
    "\n",
    "def embed_data(mime, data):\n",
    "    \"\"\"Embeds data as an html tag with a data-url.\"\"\"\n",
    "    b64 = base64.b64encode(data).decode()\n",
    "    if mime.startswith('image'):\n",
    "        tag = f'<img src=\"data:{mime};base64,{b64}\"/>'\n",
    "    elif mime.startswith('video'):\n",
    "        tag = textwrap.dedent(f\"\"\"\n",
    "            <video width=\"640\" height=\"480\" controls>\n",
    "              <source src=\"data:{mime};base64,{b64}\" type=\"video/mp4\">\n",
    "              Your browser does not support the video tag.\n",
    "            </video>\n",
    "            \"\"\")\n",
    "    else:\n",
    "        raise ValueError('Images and Video only.')\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "def embed_file(path):\n",
    "    \"\"\"Embeds a file in the notebook as an html tag with a data-url.\"\"\"\n",
    "    path = pathlib.Path(path)\n",
    "    mime, unused_encoding = mimetypes.guess_type(str(path))\n",
    "    data = path.read_bytes()\n",
    "    return embed_data(mime, data)\n",
    "\n",
    "embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiments\n",
    "\n",
    "The work that can be done here broadly falls into three main directions:\n",
    "- *Freeze* the network, work on the dataset:\n",
    "  - In this direction, most of your work is to gather datasets, and improve the ease of use. Are you able to develop a suite of tools that would allow you to handle datasets more easily? (In this case, the images are already cropped and the same size, which already takes some work! It would be nice to integrate tools that allow you to make this part of the work more streamlined: put any images in a folder, and a Python script crops them, etc.)? It might be worth looking into [data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) (inject randomness into your image dataset, [this tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix) uses that).\n",
    "  - It would be interesting to train GANs on generative images! You might end up with really distorted versions of what you started with.\n",
    "  - It's likely that people have trained GANs on spectrograms, as we see now with diffusion, but it might be a real fun thing to try?\n",
    "  - The image used for the week on text is a [book project by Allisson Parrish](https://www.aleator.press/releases/wendit-tnce-inf) that uses GANs to generate images of (unreadable) poems!\n",
    "  - Also, people have created loops where they train GANs on their own outputs, which creates distortions that may be worth exploring.\n",
    "- *Freeze* the dataset, work on the network:\n",
    "  - Maybe there's one dataset that's really your focus, or you're happy to work with established material, or the whole data processing feels boring? You might then want to look into fiddling with the model, and gather tricks (for instance: do you see an improvement if you normalise your images to be between [0,1] instead of [-1,1], like here (your Generator will have to have a `sigmoid` rather than a `tanh` as its last layer)? Then of course there's the network themselves, where all sorts of parameters can be tweaked, from the number of layers, to the strides of the convolution...\n",
    "  - **Note:** experimenting at a technical level with GANs (like with other things) can be a confusing rabbit hole. My recommendations are: make sure you have stable resources (e.g. you own a GPU or pay for Colab Pro), and try and make your net/dataset/experiments *as small/easy as possible*, so you can make a lot of them, get an inuition of what works and what doesn't. Perfect results really aren't the goal here, and it's never good for your momentum to have to wait hours or days before training finishes!\n",
    "  - How do you document this process of experimentation? You would probably need to save the various parameters of your experimentation (for yourself and, perhaps, the viewer), and associate that with some images generated at this point.\n",
    "- *Freeze* both network and dataset, and try to use the network, or its output, in unexpected ways: one could imagine just training this network, or using a top-level StyleGAN (see below), and using the resulting images in some way, as material for something else? \n",
    "\n",
    "\n",
    "## The State of the Art\n",
    "\n",
    "The field has now moved away from GANs, as Diffusion has gained in popularity. The best results have probably been achieved by [Nvidia's StyleGan 3](https://nvlabs.github.io/stylegan3/) ([repo](https://github.com/NVlabs/stylegan3)) (both written in PyTorch). Check the [StyleGAN 3 notebook](10_models_1_stylegan3.ipynb) to check it out (on Colab!).\n",
    "\n",
    "Another interesting option to look into is lucidrains' [Lightweight GAN](https://github.com/lucidrains/lightweight-gan) implementation.\n",
    "\n",
    "## Zoos: list of all GAN variants\n",
    "\n",
    "When it comes to GANs, just like Diffusion now, the explosion has been so enormous it is rather difficult (impossible?) to keep up:\n",
    "\n",
    "- [Avinash Hindupur, \"The GAN Zoo\"](https://github.com/hindupuravinash/the-gan-zoo)\n",
    "- [Jihye Back, \"GAN-Zoos\"](https://happy-jihye.github.io/gan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes / Tricks\n",
    "\n",
    "More resources worth checking: [Soumith Chintala, \"How to Train a GAN? Tips and tricks to make GANs work\"](https://github.com/soumith/ganhacks) (and [video](https://www.youtube.com/watch?v=X1mUN6dD8uE), as well as [Goodfellow's workshop](https://www.youtube.com/watch?v=HGYYEUSm-0Q)). This is summarised [in this part of a long course](https://www.youtube.com/watch?v=_cUdjPdbldQ&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=153). To go deeper still, there's [this paper](https://arxiv.org/abs/1606.03498), and a [GAN guide](https://github.com/garridoq/gan-guide), and the [Art using GANs](https://github.com/Kaustubh1Verma/Art-using-GANs) repo.\n",
    "\n",
    "Here is a summary of some of the tricks Chollet mentions in his book, that are used in this implementation:\n",
    "\n",
    "- Sample from the latent space using a **normal distribution** (Gaussian), not a uniform one;\n",
    "- GANs are likely to get stuck in all sorts of ways (it's an unstable, dynamic equilibrium): we introduce **random noise** to the labels for the discriminator to prevent this (called label smoothing);\n",
    "- Sparse gradients can hinder GAN training, remedy: **strided convolutions** for downsampling instead of max pooling, and the **`LeakyReLU`** instead of `ReLu`;\n",
    "- To avoid checkerboard artifacts caused by unequal coverage of the pixel space in the generator, use a kernel size **divisible by the stride size** with strided `Conv2DTranspose` or `Conv2D`. [In this implementation, we avoid those and use `UpSamling2D` followed by a `Conv2D`].\n",
    "\n",
    "<small>*Deep Learning With Python*, 2<sup>nd</sup> ed., p.404</small>\n",
    "\n",
    "Note also that, as is mentioned by Chintala (see lecture above), the labels for true/fake are reversed from the original formulation (here 0 is true, 1 is fake), that is said to improve stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "cc9c83ea0cbc10bff8212b234c7b05dbba13e500ad11c2ffb95769d4a1c2136e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
